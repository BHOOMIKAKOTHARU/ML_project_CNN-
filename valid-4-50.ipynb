{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54b6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,Reshape\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e588c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba0f6ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/kotha/Desktop/matplot/eeg/files/S001\\\\S001R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S002\\\\S002R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S003\\\\S003R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S004\\\\S004R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S005\\\\S005R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S006\\\\S006R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S007\\\\S007R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S008\\\\S008R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S009\\\\S009R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S010\\\\S010R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S011\\\\S011R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S012\\\\S012R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S013\\\\S013R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S014\\\\S014R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S015\\\\S015R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S016\\\\S016R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S017\\\\S017R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S018\\\\S018R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S019\\\\S019R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S020\\\\S020R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S021\\\\S021R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S022\\\\S022R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S023\\\\S023R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S024\\\\S024R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S025\\\\S025R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S026\\\\S026R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S027\\\\S027R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S028\\\\S028R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S029\\\\S029R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S030\\\\S030R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S031\\\\S031R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S032\\\\S032R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S033\\\\S033R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S034\\\\S034R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S035\\\\S035R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S036\\\\S036R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S037\\\\S037R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S038\\\\S038R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S039\\\\S039R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S040\\\\S040R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S041\\\\S041R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S042\\\\S042R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S043\\\\S043R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S044\\\\S044R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S045\\\\S045R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S046\\\\S046R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S047\\\\S047R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S048\\\\S048R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S049\\\\S049R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S050\\\\S050R05.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=[]\n",
    "test=[]\n",
    "for i in range(1,no_of_patients+1):\n",
    "    files=glob('C:/Users/kotha/Desktop/matplot/eeg/files/S'+str(str(i).zfill(3))+'/*.edf')\n",
    "    files=files[4:5]\n",
    "    train+=files\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045c4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "#     display(eeg_df)\n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a41d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe[dataframe.columns[:-1]].values\n",
    "    y=dataframe[dataframe.columns[-1]].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9885e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)\n",
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)\n",
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)  \n",
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid\n",
    "display(xtrain)\n",
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9c1f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ytemp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b27f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential()\n",
    "#model.add(Reshape((64,1),input_shape=(64,)))\n",
    "model.add(Reshape((64,1)))\n",
    "model.add(Conv1D(32, kernel_size=11, activation='relu',input_shape=(50,12000,64)))\n",
    "model.add(Conv1D(64, kernel_size=11, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(64, kernel_size=11, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87845b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18750/18750 [==============================] - 206s 11ms/step - loss: 0.5336 - val_loss: 0.9945\n",
      "Epoch 2/10\n",
      "18750/18750 [==============================] - 202s 11ms/step - loss: 0.2220 - val_loss: 1.0710\n",
      "Epoch 3/10\n",
      "18750/18750 [==============================] - 195s 10ms/step - loss: 0.1818 - val_loss: 1.0903\n",
      "Epoch 4/10\n",
      "18750/18750 [==============================] - 185s 10ms/step - loss: 0.1626 - val_loss: 1.0735\n",
      "Epoch 5/10\n",
      "18750/18750 [==============================] - 169s 9ms/step - loss: 0.1507 - val_loss: 1.1749\n",
      "Epoch 6/10\n",
      "18750/18750 [==============================] - 165s 9ms/step - loss: 0.1422 - val_loss: 1.2613\n",
      "Epoch 7/10\n",
      "18750/18750 [==============================] - 162s 9ms/step - loss: 0.1358 - val_loss: 1.4651\n",
      "Epoch 8/10\n",
      "18750/18750 [==============================] - 157s 8ms/step - loss: 0.1332 - val_loss: 1.1467\n",
      "Epoch 9/10\n",
      "18750/18750 [==============================] - 3230s 172ms/step - loss: 0.1278 - val_loss: 1.4265\n",
      "Epoch 10/10\n",
      "18750/18750 [==============================] - 137s 7ms/step - loss: 0.1245 - val_loss: 1.4516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21f8c936ec8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9262b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 64, 1)             0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 54, 32)            384       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 44, 64)            22592     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 22, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 12, 64)            45120     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 6, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 384)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               49280     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                6450      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 123,826\n",
      "Trainable params: 123,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0498fb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 9s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.70      1499\n",
      "           1       0.71      0.84      0.77      1499\n",
      "           2       0.82      0.86      0.84      1499\n",
      "           3       0.87      0.89      0.88      1499\n",
      "           4       0.98      0.98      0.98      1499\n",
      "           5       0.97      0.87      0.92      1499\n",
      "           6       0.96      0.94      0.95      1499\n",
      "           7       0.93      0.95      0.94      1499\n",
      "           8       0.88      0.82      0.85      1499\n",
      "           9       0.70      0.74      0.72      1499\n",
      "          10       0.88      0.95      0.91      1499\n",
      "          11       0.94      0.99      0.96      1499\n",
      "          12       0.89      0.85      0.87      1499\n",
      "          13       0.84      0.93      0.88      1499\n",
      "          14       0.60      0.84      0.70      1499\n",
      "          15       0.94      0.93      0.93      1499\n",
      "          16       0.87      0.92      0.90      1499\n",
      "          17       0.93      0.77      0.84      1499\n",
      "          18       0.70      0.79      0.74      1499\n",
      "          19       0.72      0.35      0.47      1499\n",
      "          20       0.99      0.95      0.97      1499\n",
      "          21       0.90      0.90      0.90      1499\n",
      "          22       0.65      0.73      0.69      1499\n",
      "          23       0.98      0.98      0.98      1499\n",
      "          24       0.89      0.98      0.93      1499\n",
      "          25       0.50      0.59      0.54      1499\n",
      "          26       0.81      0.83      0.82      1499\n",
      "          27       0.81      0.46      0.58      1499\n",
      "          28       0.73      0.83      0.78      1499\n",
      "          29       0.98      0.88      0.93      1499\n",
      "          30       0.87      0.78      0.82      1499\n",
      "          31       0.77      0.94      0.85      1499\n",
      "          32       0.92      0.79      0.85      1499\n",
      "          33       0.84      0.82      0.83      1499\n",
      "          34       0.99      0.99      0.99      1499\n",
      "          35       0.85      0.59      0.70      1499\n",
      "          36       0.85      0.87      0.86      1499\n",
      "          37       0.69      0.85      0.76      1499\n",
      "          38       0.76      0.44      0.56      1499\n",
      "          39       0.71      0.86      0.78      1499\n",
      "          40       0.93      0.96      0.95      1499\n",
      "          41       0.90      0.85      0.87      1499\n",
      "          42       0.65      0.85      0.73      1499\n",
      "          43       0.88      0.86      0.87      1499\n",
      "          44       0.88      0.83      0.85      1499\n",
      "          45       0.69      0.89      0.78      1499\n",
      "          46       0.68      0.29      0.41      1499\n",
      "          47       0.89      0.89      0.89      1499\n",
      "          48       0.75      0.92      0.82      1499\n",
      "          49       0.73      0.83      0.77      1499\n",
      "\n",
      "    accuracy                           0.82     74950\n",
      "   macro avg       0.83      0.82      0.82     74950\n",
      "weighted avg       0.83      0.82      0.82     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(xt)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb49349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343/2343 [==============================] - 8s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.25      0.32      1499\n",
      "           1       0.53      0.57      0.55      1499\n",
      "           2       0.83      0.92      0.87      1499\n",
      "           3       0.94      0.87      0.90      1499\n",
      "           4       0.97      0.98      0.97      1499\n",
      "           5       0.92      0.82      0.86      1499\n",
      "           6       0.95      0.91      0.93      1499\n",
      "           7       0.90      0.91      0.91      1499\n",
      "           8       0.86      0.71      0.78      1499\n",
      "           9       0.80      0.78      0.79      1499\n",
      "          10       0.78      0.94      0.85      1499\n",
      "          11       0.88      0.98      0.93      1499\n",
      "          12       0.85      0.84      0.85      1499\n",
      "          13       0.75      0.96      0.84      1499\n",
      "          14       0.51      0.85      0.64      1499\n",
      "          15       0.96      0.87      0.92      1499\n",
      "          16       0.72      0.82      0.76      1499\n",
      "          17       0.85      0.86      0.86      1499\n",
      "          18       0.81      0.89      0.85      1499\n",
      "          19       0.81      0.39      0.53      1499\n",
      "          20       0.96      0.96      0.96      1499\n",
      "          21       0.84      0.87      0.85      1499\n",
      "          22       0.62      0.67      0.65      1499\n",
      "          23       0.93      0.99      0.96      1499\n",
      "          24       0.74      0.97      0.84      1499\n",
      "          25       0.92      0.49      0.64      1499\n",
      "          26       0.79      0.53      0.63      1499\n",
      "          27       0.83      0.56      0.67      1499\n",
      "          28       0.81      0.68      0.74      1499\n",
      "          29       0.91      0.59      0.71      1499\n",
      "          30       0.84      0.79      0.81      1499\n",
      "          31       0.77      0.93      0.84      1499\n",
      "          32       0.85      0.90      0.87      1499\n",
      "          33       0.76      0.74      0.75      1499\n",
      "          34       0.96      1.00      0.98      1499\n",
      "          35       0.86      0.63      0.73      1499\n",
      "          36       0.82      0.96      0.89      1499\n",
      "          37       0.68      0.85      0.76      1499\n",
      "          38       0.70      0.68      0.69      1499\n",
      "          39       0.75      0.86      0.80      1499\n",
      "          40       0.86      0.48      0.62      1499\n",
      "          41       0.91      0.88      0.89      1499\n",
      "          42       0.67      0.84      0.75      1499\n",
      "          43       0.81      0.92      0.86      1499\n",
      "          44       0.88      0.80      0.84      1499\n",
      "          45       0.63      0.77      0.69      1499\n",
      "          46       0.60      0.54      0.57      1499\n",
      "          47       0.83      0.84      0.84      1499\n",
      "          48       0.66      0.92      0.77      1499\n",
      "          49       0.73      0.79      0.76      1499\n",
      "\n",
      "    accuracy                           0.79     74950\n",
      "   macro avg       0.80      0.79      0.79     74950\n",
      "weighted avg       0.80      0.79      0.79     74950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''y_pred=model.evaluate(xv,yv)\n",
    "print(y_pred)\n",
    "print(classification_report(yv,y_pred_classes))'''\n",
    "y_pred=model.predict(xv)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(yv,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62de9e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 71s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     12000\n",
      "           1       0.91      0.98      0.94     12000\n",
      "           2       0.96      0.96      0.96     12000\n",
      "           3       0.99      0.99      0.99     12000\n",
      "           4       0.99      0.99      0.99     12000\n",
      "           5       0.99      0.98      0.99     12000\n",
      "           6       0.98      0.95      0.97     12000\n",
      "           7       0.99      0.99      0.99     12000\n",
      "           8       0.99      0.94      0.96     12000\n",
      "           9       0.99      0.99      0.99     12000\n",
      "          10       0.97      1.00      0.98     12000\n",
      "          11       0.98      1.00      0.99     12000\n",
      "          12       0.97      0.87      0.92     12000\n",
      "          13       0.96      0.98      0.97     12000\n",
      "          14       0.87      0.93      0.90     12000\n",
      "          15       0.99      0.99      0.99     12000\n",
      "          16       0.98      0.98      0.98     12000\n",
      "          17       0.99      0.99      0.99     12000\n",
      "          18       0.93      0.94      0.93     12000\n",
      "          19       0.99      0.97      0.98     12000\n",
      "          20       1.00      0.99      0.99     12000\n",
      "          21       0.99      0.97      0.98     12000\n",
      "          22       0.87      0.91      0.89     12000\n",
      "          23       0.99      0.99      0.99     12000\n",
      "          24       0.98      1.00      0.99     12000\n",
      "          25       0.99      0.99      0.99     12000\n",
      "          26       0.98      0.99      0.98     12000\n",
      "          27       0.98      0.96      0.97     12000\n",
      "          28       0.98      0.98      0.98     12000\n",
      "          29       1.00      0.99      0.99     12000\n",
      "          30       0.97      0.95      0.96     12000\n",
      "          31       0.95      0.97      0.96     12000\n",
      "          32       0.99      0.94      0.96     12000\n",
      "          33       0.99      0.98      0.98     12000\n",
      "          34       0.99      1.00      0.99     12000\n",
      "          35       0.96      0.83      0.89     12000\n",
      "          36       0.97      0.99      0.98     12000\n",
      "          37       0.85      0.94      0.89     12000\n",
      "          38       0.99      0.98      0.99     12000\n",
      "          39       0.97      0.99      0.98     12000\n",
      "          40       0.99      0.99      0.99     12000\n",
      "          41       0.99      0.99      0.99     12000\n",
      "          42       0.96      0.98      0.97     12000\n",
      "          43       0.98      0.96      0.97     12000\n",
      "          44       0.96      0.93      0.94     12000\n",
      "          45       0.90      0.97      0.94     12000\n",
      "          46       0.99      0.96      0.98     12000\n",
      "          47       0.98      0.98      0.98     12000\n",
      "          48       0.94      0.94      0.94     12000\n",
      "          49       0.95      0.96      0.96     12000\n",
      "\n",
      "    accuracy                           0.97    600000\n",
      "   macro avg       0.97      0.97      0.97    600000\n",
      "weighted avg       0.97      0.97      0.97    600000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(x)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b7cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
