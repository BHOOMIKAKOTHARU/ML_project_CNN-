{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51eda0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,Reshape\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e18bdb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37e53c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/kotha/Desktop/matplot/eeg/files/S001\\\\S001R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S002\\\\S002R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S003\\\\S003R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S004\\\\S004R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S005\\\\S005R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S006\\\\S006R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S007\\\\S007R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S008\\\\S008R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S009\\\\S009R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S010\\\\S010R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S011\\\\S011R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S012\\\\S012R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S013\\\\S013R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S014\\\\S014R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S015\\\\S015R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S016\\\\S016R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S017\\\\S017R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S018\\\\S018R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S019\\\\S019R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S020\\\\S020R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S021\\\\S021R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S022\\\\S022R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S023\\\\S023R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S024\\\\S024R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S025\\\\S025R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S026\\\\S026R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S027\\\\S027R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S028\\\\S028R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S029\\\\S029R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S030\\\\S030R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S031\\\\S031R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S032\\\\S032R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S033\\\\S033R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S034\\\\S034R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S035\\\\S035R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S036\\\\S036R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S037\\\\S037R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S038\\\\S038R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S039\\\\S039R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S040\\\\S040R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S041\\\\S041R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S042\\\\S042R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S043\\\\S043R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S044\\\\S044R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S045\\\\S045R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S046\\\\S046R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S047\\\\S047R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S048\\\\S048R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S049\\\\S049R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S050\\\\S050R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S051\\\\S051R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S052\\\\S052R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S053\\\\S053R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S054\\\\S054R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S055\\\\S055R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S056\\\\S056R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S057\\\\S057R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S058\\\\S058R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S059\\\\S059R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S060\\\\S060R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S061\\\\S061R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S062\\\\S062R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S063\\\\S063R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S064\\\\S064R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S065\\\\S065R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S066\\\\S066R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S067\\\\S067R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S068\\\\S068R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S069\\\\S069R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S070\\\\S070R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S071\\\\S071R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S072\\\\S072R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S073\\\\S073R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S074\\\\S074R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S075\\\\S075R05.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=[]\n",
    "test=[]\n",
    "for i in range(1,no_of_patients+1):\n",
    "    files=glob('C:/Users/kotha/Desktop/matplot/eeg/files/S'+str(str(i).zfill(3))+'/*.edf')\n",
    "    files=files[4:5]\n",
    "    train+=files\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50dbc160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    \n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "#     display(eeg_df)\n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0591dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe[dataframe.columns[:-1]].values\n",
    "    y=dataframe[dataframe.columns[-1]].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2649fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)\n",
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)\n",
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)  \n",
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid\n",
    "display(xtrain)\n",
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc09fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential()\n",
    "#model.add(Reshape((64,1),input_shape=(64,)))\n",
    "model.add(Reshape((64,1)))\n",
    "model.add(Conv1D(32, kernel_size=11, activation='relu',input_shape=(75,12000,64)))\n",
    "model.add(Conv1D(64, kernel_size=11, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(64, kernel_size=11, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(75, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "720c3017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28125/28125 [==============================] - 264s 9ms/step - loss: 0.7142 - val_loss: 1.2868\n",
      "Epoch 2/10\n",
      "28125/28125 [==============================] - 258s 9ms/step - loss: 0.3493 - val_loss: 1.1827\n",
      "Epoch 3/10\n",
      "28125/28125 [==============================] - 260s 9ms/step - loss: 0.2943 - val_loss: 1.3044\n",
      "Epoch 4/10\n",
      "28125/28125 [==============================] - 274s 10ms/step - loss: 0.2698 - val_loss: 1.3464\n",
      "Epoch 5/10\n",
      "28125/28125 [==============================] - 269s 10ms/step - loss: 0.2549 - val_loss: 1.2666\n",
      "Epoch 6/10\n",
      "28125/28125 [==============================] - 249s 9ms/step - loss: 0.2461 - val_loss: 1.2471\n",
      "Epoch 7/10\n",
      "28125/28125 [==============================] - 290s 10ms/step - loss: 0.2384 - val_loss: 1.3894\n",
      "Epoch 8/10\n",
      "28125/28125 [==============================] - 266s 9ms/step - loss: 0.2321 - val_loss: 1.3584\n",
      "Epoch 9/10\n",
      "28125/28125 [==============================] - 274s 10ms/step - loss: 0.2301 - val_loss: 1.3691\n",
      "Epoch 10/10\n",
      "28125/28125 [==============================] - 276s 10ms/step - loss: 0.2246 - val_loss: 1.4660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2501230c408>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f4beab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28125/28125 [==============================] - 107s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90     12000\n",
      "           1       0.97      0.92      0.94     12000\n",
      "           2       0.94      0.95      0.95     12000\n",
      "           3       0.98      0.98      0.98     12000\n",
      "           4       0.99      0.98      0.99     12000\n",
      "           5       0.99      0.98      0.99     12000\n",
      "           6       0.95      0.97      0.96     12000\n",
      "           7       0.97      0.99      0.98     12000\n",
      "           8       0.97      0.95      0.96     12000\n",
      "           9       0.98      0.97      0.98     12000\n",
      "          10       0.97      0.99      0.98     12000\n",
      "          11       0.99      0.99      0.99     12000\n",
      "          12       0.95      0.83      0.88     12000\n",
      "          13       0.95      0.94      0.95     12000\n",
      "          14       0.87      0.82      0.84     12000\n",
      "          15       0.97      0.99      0.98     12000\n",
      "          16       0.99      0.96      0.98     12000\n",
      "          17       0.98      0.98      0.98     12000\n",
      "          18       0.84      0.93      0.88     12000\n",
      "          19       0.99      0.96      0.97     12000\n",
      "          20       1.00      0.99      1.00     12000\n",
      "          21       0.98      0.98      0.98     12000\n",
      "          22       0.96      0.76      0.85     12000\n",
      "          23       0.98      0.99      0.98     12000\n",
      "          24       0.99      0.98      0.99     12000\n",
      "          25       0.99      0.98      0.99     12000\n",
      "          26       0.99      0.96      0.98     12000\n",
      "          27       0.96      0.98      0.97     12000\n",
      "          28       0.95      0.96      0.95     12000\n",
      "          29       0.98      1.00      0.99     12000\n",
      "          30       0.95      0.94      0.95     12000\n",
      "          31       0.96      0.96      0.96     12000\n",
      "          32       0.99      0.93      0.96     12000\n",
      "          33       0.98      0.97      0.98     12000\n",
      "          34       0.97      1.00      0.98     12000\n",
      "          35       0.87      0.86      0.87     12000\n",
      "          36       0.99      0.96      0.98     12000\n",
      "          37       0.89      0.81      0.85     12000\n",
      "          38       0.96      0.99      0.98     12000\n",
      "          39       0.94      0.95      0.95     12000\n",
      "          40       0.99      0.99      0.99     12000\n",
      "          41       0.99      0.99      0.99     12000\n",
      "          42       0.96      0.95      0.96     12000\n",
      "          43       0.93      0.97      0.95     12000\n",
      "          44       0.97      0.89      0.93     12000\n",
      "          45       0.93      0.94      0.93     12000\n",
      "          46       0.95      0.98      0.96     12000\n",
      "          47       0.98      0.99      0.99     12000\n",
      "          48       0.87      0.94      0.91     12000\n",
      "          49       0.93      0.95      0.94     12000\n",
      "          50       0.98      0.95      0.96     12000\n",
      "          51       0.94      0.98      0.96     12000\n",
      "          52       0.87      0.91      0.89     12000\n",
      "          53       0.96      0.93      0.94     12000\n",
      "          54       0.98      0.98      0.98     12000\n",
      "          55       0.81      0.91      0.86     12000\n",
      "          56       0.95      0.91      0.93     12000\n",
      "          57       0.95      0.99      0.97     12000\n",
      "          58       1.00      1.00      1.00     12000\n",
      "          59       0.96      0.97      0.97     12000\n",
      "          60       0.94      0.94      0.94     12000\n",
      "          61       0.98      0.98      0.98     12000\n",
      "          62       0.95      0.98      0.96     12000\n",
      "          63       0.97      0.98      0.97     12000\n",
      "          64       0.94      0.98      0.96     12000\n",
      "          65       0.95      0.98      0.96     12000\n",
      "          66       0.91      0.94      0.92     12000\n",
      "          67       0.99      0.99      0.99     12000\n",
      "          68       0.95      0.95      0.95     12000\n",
      "          69       0.82      0.94      0.88     12000\n",
      "          70       0.98      0.99      0.98     12000\n",
      "          71       0.96      0.98      0.97     12000\n",
      "          72       0.99      0.95      0.97     12000\n",
      "          73       0.95      0.99      0.97     12000\n",
      "          74       0.87      0.88      0.88     12000\n",
      "\n",
      "    accuracy                           0.95    900000\n",
      "   macro avg       0.95      0.95      0.95    900000\n",
      "weighted avg       0.95      0.95      0.95    900000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(x)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f05dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 16s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.16      0.23      1499\n",
      "           1       0.85      0.41      0.55      1499\n",
      "           2       0.79      0.91      0.85      1499\n",
      "           3       0.90      0.86      0.88      1499\n",
      "           4       0.95      0.95      0.95      1499\n",
      "           5       0.97      0.91      0.94      1499\n",
      "           6       0.88      0.94      0.91      1499\n",
      "           7       0.93      0.98      0.95      1499\n",
      "           8       0.74      0.87      0.80      1499\n",
      "           9       0.77      0.76      0.76      1499\n",
      "          10       0.86      0.92      0.89      1499\n",
      "          11       0.96      0.97      0.96      1499\n",
      "          12       0.85      0.82      0.83      1499\n",
      "          13       0.86      0.89      0.88      1499\n",
      "          14       0.53      0.68      0.59      1499\n",
      "          15       0.90      0.90      0.90      1499\n",
      "          16       0.90      0.90      0.90      1499\n",
      "          17       0.82      0.84      0.83      1499\n",
      "          18       0.64      0.90      0.75      1499\n",
      "          19       0.82      0.36      0.50      1499\n",
      "          20       0.98      0.99      0.98      1499\n",
      "          21       0.87      0.88      0.87      1499\n",
      "          22       0.79      0.50      0.61      1499\n",
      "          23       0.91      0.99      0.95      1499\n",
      "          24       0.82      0.91      0.86      1499\n",
      "          25       0.89      0.35      0.50      1499\n",
      "          26       0.82      0.37      0.51      1499\n",
      "          27       0.75      0.69      0.72      1499\n",
      "          28       0.58      0.69      0.63      1499\n",
      "          29       0.78      0.66      0.72      1499\n",
      "          30       0.83      0.86      0.84      1499\n",
      "          31       0.83      0.91      0.87      1499\n",
      "          32       0.89      0.92      0.91      1499\n",
      "          33       0.82      0.83      0.83      1499\n",
      "          34       0.95      0.99      0.97      1499\n",
      "          35       0.73      0.74      0.74      1499\n",
      "          36       0.93      0.81      0.87      1499\n",
      "          37       0.71      0.72      0.72      1499\n",
      "          38       0.63      0.56      0.59      1499\n",
      "          39       0.62      0.74      0.67      1499\n",
      "          40       0.92      0.48      0.63      1499\n",
      "          41       0.89      0.87      0.88      1499\n",
      "          42       0.67      0.69      0.68      1499\n",
      "          43       0.73      0.94      0.82      1499\n",
      "          44       0.91      0.76      0.83      1499\n",
      "          45       0.65      0.74      0.69      1499\n",
      "          46       0.41      0.64      0.50      1499\n",
      "          47       0.92      0.83      0.88      1499\n",
      "          48       0.74      0.90      0.81      1499\n",
      "          49       0.71      0.78      0.74      1499\n",
      "          50       0.68      0.36      0.47      1499\n",
      "          51       0.59      0.76      0.66      1499\n",
      "          52       0.75      0.83      0.79      1499\n",
      "          53       0.85      0.78      0.81      1499\n",
      "          54       0.81      0.67      0.73      1499\n",
      "          55       0.69      0.89      0.78      1499\n",
      "          56       0.74      0.59      0.65      1499\n",
      "          57       0.84      0.97      0.90      1499\n",
      "          58       0.99      0.82      0.90      1499\n",
      "          59       0.64      0.53      0.58      1499\n",
      "          60       0.78      0.63      0.70      1499\n",
      "          61       0.77      0.32      0.45      1499\n",
      "          62       0.90      0.97      0.93      1499\n",
      "          63       0.77      0.86      0.81      1499\n",
      "          64       0.66      0.72      0.69      1499\n",
      "          65       0.79      0.86      0.82      1499\n",
      "          66       0.64      0.88      0.74      1499\n",
      "          67       0.90      0.64      0.74      1499\n",
      "          68       0.64      0.92      0.76      1499\n",
      "          69       0.40      0.69      0.51      1499\n",
      "          70       0.88      0.79      0.83      1499\n",
      "          71       0.54      0.63      0.58      1499\n",
      "          72       0.91      0.89      0.90      1499\n",
      "          73       0.75      0.98      0.85      1499\n",
      "          74       0.57      0.78      0.66      1499\n",
      "\n",
      "    accuracy                           0.77    112425\n",
      "   macro avg       0.78      0.77      0.76    112425\n",
      "weighted avg       0.78      0.77      0.76    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(xv)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(yv,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bd1c22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514/3514 [==============================] - 16s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.66      0.73      1499\n",
      "           1       0.91      0.61      0.73      1499\n",
      "           2       0.83      0.84      0.83      1499\n",
      "           3       0.92      0.86      0.89      1499\n",
      "           4       0.95      0.98      0.96      1499\n",
      "           5       0.98      0.90      0.94      1499\n",
      "           6       0.88      0.97      0.92      1499\n",
      "           7       0.94      0.96      0.95      1499\n",
      "           8       0.80      0.88      0.84      1499\n",
      "           9       0.81      0.73      0.77      1499\n",
      "          10       0.83      0.99      0.90      1499\n",
      "          11       0.97      0.98      0.97      1499\n",
      "          12       0.87      0.83      0.85      1499\n",
      "          13       0.86      0.86      0.86      1499\n",
      "          14       0.69      0.68      0.69      1499\n",
      "          15       0.90      0.95      0.92      1499\n",
      "          16       0.91      0.95      0.93      1499\n",
      "          17       0.79      0.79      0.79      1499\n",
      "          18       0.60      0.80      0.68      1499\n",
      "          19       0.77      0.34      0.47      1499\n",
      "          20       0.99      0.98      0.98      1499\n",
      "          21       0.91      0.92      0.91      1499\n",
      "          22       0.81      0.54      0.65      1499\n",
      "          23       0.94      0.99      0.96      1499\n",
      "          24       0.82      0.97      0.89      1499\n",
      "          25       0.63      0.45      0.52      1499\n",
      "          26       0.94      0.66      0.77      1499\n",
      "          27       0.66      0.58      0.62      1499\n",
      "          28       0.60      0.86      0.71      1499\n",
      "          29       0.93      0.95      0.94      1499\n",
      "          30       0.87      0.85      0.86      1499\n",
      "          31       0.82      0.91      0.86      1499\n",
      "          32       0.82      0.81      0.82      1499\n",
      "          33       0.90      0.84      0.87      1499\n",
      "          34       0.97      0.98      0.97      1499\n",
      "          35       0.72      0.71      0.72      1499\n",
      "          36       0.89      0.85      0.87      1499\n",
      "          37       0.80      0.72      0.76      1499\n",
      "          38       0.55      0.80      0.65      1499\n",
      "          39       0.65      0.71      0.68      1499\n",
      "          40       0.96      0.79      0.87      1499\n",
      "          41       0.90      0.78      0.84      1499\n",
      "          42       0.70      0.64      0.67      1499\n",
      "          43       0.79      0.90      0.85      1499\n",
      "          44       0.94      0.80      0.86      1499\n",
      "          45       0.78      0.79      0.79      1499\n",
      "          46       0.32      0.37      0.34      1499\n",
      "          47       0.94      0.77      0.85      1499\n",
      "          48       0.76      0.94      0.84      1499\n",
      "          49       0.71      0.87      0.78      1499\n",
      "          50       0.67      0.55      0.60      1499\n",
      "          51       0.70      0.79      0.74      1499\n",
      "          52       0.67      0.90      0.77      1499\n",
      "          53       0.89      0.72      0.79      1499\n",
      "          54       0.73      0.72      0.72      1499\n",
      "          55       0.71      0.86      0.78      1499\n",
      "          56       0.75      0.62      0.68      1499\n",
      "          57       0.86      0.97      0.92      1499\n",
      "          58       0.99      0.76      0.86      1499\n",
      "          59       0.71      0.72      0.72      1499\n",
      "          60       0.75      0.82      0.78      1499\n",
      "          61       0.94      0.79      0.86      1499\n",
      "          62       0.92      0.96      0.94      1499\n",
      "          63       0.72      0.62      0.67      1499\n",
      "          64       0.73      0.76      0.74      1499\n",
      "          65       0.88      0.95      0.91      1499\n",
      "          66       0.72      0.73      0.72      1499\n",
      "          67       0.77      0.41      0.54      1499\n",
      "          68       0.88      0.92      0.90      1499\n",
      "          69       0.57      0.87      0.69      1499\n",
      "          70       0.88      0.58      0.70      1499\n",
      "          71       0.54      0.55      0.54      1499\n",
      "          72       0.96      0.90      0.92      1499\n",
      "          73       0.66      0.94      0.77      1499\n",
      "          74       0.54      0.69      0.60      1499\n",
      "\n",
      "    accuracy                           0.79    112425\n",
      "   macro avg       0.80      0.79      0.79    112425\n",
      "weighted avg       0.80      0.79      0.79    112425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(xt)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f032c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
