{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7342c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,Reshape\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed556a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_patients=109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47e5ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/kotha/Desktop/matplot/eeg/files/S001\\\\S001R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S002\\\\S002R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S003\\\\S003R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S004\\\\S004R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S005\\\\S005R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S006\\\\S006R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S007\\\\S007R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S008\\\\S008R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S009\\\\S009R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S010\\\\S010R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S011\\\\S011R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S012\\\\S012R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S013\\\\S013R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S014\\\\S014R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S015\\\\S015R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S016\\\\S016R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S017\\\\S017R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S018\\\\S018R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S019\\\\S019R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S020\\\\S020R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S021\\\\S021R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S022\\\\S022R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S023\\\\S023R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S024\\\\S024R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S025\\\\S025R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S026\\\\S026R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S027\\\\S027R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S028\\\\S028R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S029\\\\S029R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S030\\\\S030R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S031\\\\S031R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S032\\\\S032R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S033\\\\S033R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S034\\\\S034R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S035\\\\S035R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S036\\\\S036R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S037\\\\S037R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S038\\\\S038R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S039\\\\S039R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S040\\\\S040R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S041\\\\S041R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S042\\\\S042R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S043\\\\S043R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S044\\\\S044R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S045\\\\S045R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S046\\\\S046R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S047\\\\S047R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S048\\\\S048R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S049\\\\S049R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S050\\\\S050R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S051\\\\S051R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S052\\\\S052R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S053\\\\S053R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S054\\\\S054R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S055\\\\S055R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S056\\\\S056R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S057\\\\S057R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S058\\\\S058R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S059\\\\S059R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S060\\\\S060R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S061\\\\S061R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S062\\\\S062R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S063\\\\S063R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S064\\\\S064R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S065\\\\S065R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S066\\\\S066R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S067\\\\S067R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S068\\\\S068R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S069\\\\S069R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S070\\\\S070R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S071\\\\S071R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S072\\\\S072R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S073\\\\S073R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S074\\\\S074R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S075\\\\S075R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S076\\\\S076R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S077\\\\S077R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S078\\\\S078R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S079\\\\S079R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S080\\\\S080R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S081\\\\S081R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S082\\\\S082R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S083\\\\S083R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S084\\\\S084R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S085\\\\S085R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S086\\\\S086R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S087\\\\S087R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S088\\\\S088R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S089\\\\S089R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S090\\\\S090R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S091\\\\S091R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S092\\\\S092R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S093\\\\S093R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S094\\\\S094R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S095\\\\S095R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S096\\\\S096R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S097\\\\S097R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S098\\\\S098R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S099\\\\S099R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S100\\\\S100R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S101\\\\S101R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S102\\\\S102R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S103\\\\S103R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S104\\\\S104R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S105\\\\S105R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S106\\\\S106R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S107\\\\S107R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S108\\\\S108R05.edf',\n",
       " 'C:/Users/kotha/Desktop/matplot/eeg/files/S109\\\\S109R05.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=[]\n",
    "test=[]\n",
    "for i in range(1,no_of_patients+1):\n",
    "    files=glob('C:/Users/kotha/Desktop/matplot/eeg/files/S'+str(str(i).zfill(3))+'/*.edf')\n",
    "    files=files[4:5]\n",
    "    train+=files\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b97577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(i,train_split,valid_split):\n",
    "    raw = mne.io.read_raw_edf(i, preload=True)\n",
    "    eeg_data = raw.get_data()\n",
    "    #eeg_data=eeg_data[:64]\n",
    "    eeg_channels = [f'Channel_{i}' for i in range(eeg_data.shape[0])]\n",
    "    eeg_df = pd.DataFrame(data=eeg_data.T, columns=eeg_channels)\n",
    "#     display(eeg_df)\n",
    "    eeg_df = eeg_df.iloc[:15000]\n",
    "    idx1= int(train_split*(len(eeg_df)))\n",
    "    idx2= int(train_split*(len(eeg_df)))+1\n",
    "    eeg_df1=eeg_df.iloc[:idx1]\n",
    "    eeg_df2=eeg_df.iloc[idx2:]\n",
    "    idx3=int(valid_split*(len(eeg_df2)))\n",
    "    idx4=int(valid_split*(len(eeg_df2)))+1\n",
    "    eeg_df3=eeg_df2.iloc[:idx3]\n",
    "    eeg_df4=eeg_df2.iloc[idx4:]\n",
    "    return eeg_df1,eeg_df3,eeg_df4,len(eeg_df1),len(eeg_df3),len(eeg_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defb54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe):\n",
    "    x=dataframe[dataframe.columns[:-1]].values\n",
    "    y=dataframe[dataframe.columns[-1]].values\n",
    "    scaler =StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff9cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "xtemp1=[]\n",
    "xtemp2=[]\n",
    "xtemp3=[]\n",
    "ytemp1=[]\n",
    "ytemp2=[]\n",
    "ytemp3=[]\n",
    "for i in range(no_of_patients):\n",
    "    xtr,xte,xval,ytr,yte,yval=read_data(train[i],0.8,0.5) # xtr=xtrain, xte=xtest, ytr=ytrain, yte=ytest.\n",
    "    xtemp1.append(xtr)\n",
    "    xtemp2.append(xte)\n",
    "    xtemp3.append(xval)\n",
    "    ytemp1.append(ytr)\n",
    "    ytemp2.append(yte)\n",
    "    ytemp3.append(yval)\n",
    "xtrain = pd.concat([xtemp1[i] for i in range(0, len(xtemp1))], ignore_index=True)\n",
    "xtest = pd.concat([xtemp2[i] for i in range(0, len(xtemp2))], ignore_index=True)\n",
    "xvalid=pd.concat([xtemp3[i] for i in range(0,len(xtemp3))],ignore_index=True)\n",
    "ytrain=[]\n",
    "for i in range(len(ytemp1)):\n",
    "    for j in range(ytemp1[i-1]):\n",
    "        ytrain.append(i)\n",
    "ytest=[]\n",
    "for i in range(len(ytemp2)):\n",
    "    for j in range(ytemp2[i-1]):\n",
    "        ytest.append(i)        \n",
    "yvalid=[]\n",
    "for i in range(len(ytemp3)):\n",
    "    for j in range(ytemp3[i-1]):\n",
    "        yvalid.append(i)  \n",
    "xtrain['id']=ytrain\n",
    "xtest['id']=ytest\n",
    "xvalid['id']=yvalid\n",
    "display(xtrain)\n",
    "x,y=scale_dataset(xtrain)\n",
    "xt,yt=scale_dataset(xtest)\n",
    "xv,yv=scale_dataset(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85fc69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential()\n",
    "#model.add(Reshape((64,1),input_shape=(64,)))\n",
    "model.add(Reshape((64,1)))\n",
    "model.add(Conv1D(32, kernel_size=11, activation='relu',input_shape=(109,12000,64)))\n",
    "model.add(Conv1D(64, kernel_size=11, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(64, kernel_size=11, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(109, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd97281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40648/40648 [==============================] - 413s 10ms/step - loss: 0.8164 - val_loss: 1.3868\n",
      "Epoch 2/10\n",
      "40648/40648 [==============================] - 413s 10ms/step - loss: 0.4543 - val_loss: 1.3969\n",
      "Epoch 3/10\n",
      "40648/40648 [==============================] - 411s 10ms/step - loss: 0.3979 - val_loss: 1.4278\n",
      "Epoch 4/10\n",
      "40648/40648 [==============================] - 333s 8ms/step - loss: 0.3707 - val_loss: 1.4144\n",
      "Epoch 5/10\n",
      "40648/40648 [==============================] - 333s 8ms/step - loss: 0.3558 - val_loss: 1.4316\n",
      "Epoch 6/10\n",
      "40648/40648 [==============================] - 392s 10ms/step - loss: 0.3451 - val_loss: 1.4403\n",
      "Epoch 7/10\n",
      "40648/40648 [==============================] - 494s 12ms/step - loss: 0.3370 - val_loss: 1.4604\n",
      "Epoch 8/10\n",
      "40648/40648 [==============================] - 463s 11ms/step - loss: 0.3336 - val_loss: 1.5663\n",
      "Epoch 9/10\n",
      "40648/40648 [==============================] - 420s 10ms/step - loss: 0.3288 - val_loss: 1.5178\n",
      "Epoch 10/10\n",
      "40648/40648 [==============================] - 410s 10ms/step - loss: 0.3242 - val_loss: 1.6276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x168ab2f8688>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x,y,epochs=10,validation_data=(xv,yv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70920a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 64, 1)             0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 54, 32)            384       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 44, 64)            22592     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 22, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 12, 64)            45120     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 6, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 384)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               49280     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 109)               14061     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131,437\n",
      "Trainable params: 131,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60ec7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5078/5078 [==============================] - 21s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.19      0.23      1499\n",
      "           1       0.74      0.43      0.54      1499\n",
      "           2       0.82      0.86      0.84      1499\n",
      "           3       0.88      0.88      0.88      1499\n",
      "           4       0.84      0.96      0.89      1499\n",
      "           5       0.98      0.91      0.95      1499\n",
      "           6       0.87      0.91      0.89      1499\n",
      "           7       0.95      0.89      0.92      1499\n",
      "           8       0.66      0.93      0.77      1499\n",
      "           9       0.80      0.81      0.81      1499\n",
      "          10       0.92      0.88      0.90      1499\n",
      "          11       0.95      0.98      0.96      1499\n",
      "          12       0.72      0.83      0.77      1499\n",
      "          13       0.78      0.94      0.85      1499\n",
      "          14       0.40      0.65      0.49      1499\n",
      "          15       0.94      0.86      0.89      1499\n",
      "          16       0.82      0.88      0.85      1499\n",
      "          17       0.89      0.85      0.87      1499\n",
      "          18       0.64      0.90      0.75      1499\n",
      "          19       0.86      0.49      0.63      1499\n",
      "          20       0.96      0.98      0.97      1499\n",
      "          21       0.82      0.88      0.85      1499\n",
      "          22       0.69      0.63      0.66      1499\n",
      "          23       0.81      0.99      0.89      1499\n",
      "          24       0.95      0.82      0.88      1499\n",
      "          25       0.90      0.47      0.62      1499\n",
      "          26       0.71      0.60      0.65      1499\n",
      "          27       0.40      0.68      0.50      1499\n",
      "          28       0.66      0.65      0.66      1499\n",
      "          29       0.76      0.54      0.63      1499\n",
      "          30       0.87      0.85      0.86      1499\n",
      "          31       0.62      0.90      0.74      1499\n",
      "          32       0.83      0.92      0.87      1499\n",
      "          33       0.72      0.87      0.79      1499\n",
      "          34       0.96      0.99      0.97      1499\n",
      "          35       0.69      0.64      0.66      1499\n",
      "          36       0.76      0.97      0.85      1499\n",
      "          37       0.56      0.79      0.66      1499\n",
      "          38       0.71      0.50      0.59      1499\n",
      "          39       0.58      0.70      0.63      1499\n",
      "          40       0.98      0.50      0.66      1499\n",
      "          41       0.92      0.92      0.92      1499\n",
      "          42       0.53      0.69      0.60      1499\n",
      "          43       0.74      0.91      0.82      1499\n",
      "          44       0.85      0.86      0.86      1499\n",
      "          45       0.61      0.81      0.70      1499\n",
      "          46       0.45      0.62      0.52      1499\n",
      "          47       0.81      0.69      0.74      1499\n",
      "          48       0.76      0.83      0.80      1499\n",
      "          49       0.69      0.82      0.75      1499\n",
      "          50       0.69      0.47      0.56      1499\n",
      "          51       0.67      0.62      0.65      1499\n",
      "          52       0.65      0.84      0.74      1499\n",
      "          53       0.72      0.81      0.76      1499\n",
      "          54       0.84      0.60      0.70      1499\n",
      "          55       0.80      0.78      0.79      1499\n",
      "          56       0.77      0.53      0.63      1499\n",
      "          57       0.91      0.96      0.93      1499\n",
      "          58       1.00      0.84      0.91      1499\n",
      "          59       0.67      0.47      0.55      1499\n",
      "          60       0.72      0.73      0.73      1499\n",
      "          61       0.79      0.39      0.52      1499\n",
      "          62       0.96      0.91      0.93      1499\n",
      "          63       0.88      0.73      0.80      1499\n",
      "          64       0.66      0.58      0.62      1499\n",
      "          65       0.83      0.88      0.85      1499\n",
      "          66       0.61      0.89      0.73      1499\n",
      "          67       0.91      0.58      0.71      1499\n",
      "          68       0.69      0.90      0.78      1499\n",
      "          69       0.46      0.65      0.54      1499\n",
      "          70       0.96      0.44      0.61      1499\n",
      "          71       0.45      0.58      0.51      1499\n",
      "          72       0.92      0.87      0.90      1499\n",
      "          73       0.90      0.94      0.92      1499\n",
      "          74       0.51      0.75      0.61      1499\n",
      "          75       0.95      0.92      0.94      1499\n",
      "          76       0.63      0.48      0.55      1499\n",
      "          77       0.47      0.34      0.40      1499\n",
      "          78       0.79      0.87      0.83      1499\n",
      "          79       0.70      0.85      0.77      1499\n",
      "          80       0.95      0.96      0.95      1499\n",
      "          81       0.77      0.65      0.71      1499\n",
      "          82       0.78      0.89      0.83      1499\n",
      "          83       0.98      0.79      0.87      1499\n",
      "          84       0.84      0.87      0.86      1499\n",
      "          85       0.84      0.88      0.86      1499\n",
      "          86       0.54      0.49      0.51      1499\n",
      "          87       0.43      0.39      0.41      1499\n",
      "          88       0.93      0.97      0.95      1499\n",
      "          89       0.75      0.86      0.80      1499\n",
      "          90       0.89      0.98      0.94      1499\n",
      "          91       0.65      0.82      0.73      1499\n",
      "          92       0.97      0.84      0.90      1499\n",
      "          93       1.00      1.00      1.00      1499\n",
      "          94       0.83      0.80      0.82      1499\n",
      "          95       0.69      0.69      0.69      1499\n",
      "          96       0.86      0.80      0.83      1499\n",
      "          97       0.73      0.96      0.83      1499\n",
      "          98       0.81      0.84      0.83      1499\n",
      "          99       0.68      0.84      0.75      1499\n",
      "         100       0.80      0.56      0.65      1499\n",
      "         101       0.76      0.78      0.77      1499\n",
      "         102       0.76      0.34      0.47      1499\n",
      "         103       0.38      0.15      0.21      1499\n",
      "         104       0.93      0.68      0.78      1499\n",
      "         105       0.46      0.41      0.43      1499\n",
      "         106       0.35      0.32      0.34       591\n",
      "         107       0.68      0.55      0.61      1499\n",
      "         108       0.96      0.75      0.84      1499\n",
      "\n",
      "    accuracy                           0.75    162483\n",
      "   macro avg       0.76      0.74      0.74    162483\n",
      "weighted avg       0.76      0.75      0.74    162483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(xv)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(yv,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f83e56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40648/40648 [==============================] - 171s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.89     12000\n",
      "           1       0.95      0.93      0.94     12000\n",
      "           2       0.96      0.91      0.93     12000\n",
      "           3       0.95      0.99      0.97     12000\n",
      "           4       0.97      0.98      0.97     12000\n",
      "           5       1.00      0.97      0.98     12000\n",
      "           6       0.97      0.95      0.96     12000\n",
      "           7       0.99      0.98      0.98     12000\n",
      "           8       0.93      0.96      0.95     12000\n",
      "           9       0.95      0.97      0.96     12000\n",
      "          10       0.97      0.99      0.98     12000\n",
      "          11       0.98      0.99      0.99     12000\n",
      "          12       0.90      0.80      0.85     12000\n",
      "          13       0.92      0.94      0.93     12000\n",
      "          14       0.72      0.80      0.76     12000\n",
      "          15       0.97      0.99      0.98     12000\n",
      "          16       0.97      0.97      0.97     12000\n",
      "          17       0.98      0.96      0.97     12000\n",
      "          18       0.81      0.91      0.86     12000\n",
      "          19       0.98      0.93      0.96     12000\n",
      "          20       1.00      0.99      0.99     12000\n",
      "          21       0.96      0.96      0.96     12000\n",
      "          22       0.87      0.82      0.85     12000\n",
      "          23       0.97      0.99      0.98     12000\n",
      "          24       0.99      0.96      0.98     12000\n",
      "          25       0.99      0.98      0.98     12000\n",
      "          26       0.98      0.97      0.97     12000\n",
      "          27       0.83      0.99      0.90     12000\n",
      "          28       0.94      0.94      0.94     12000\n",
      "          29       0.99      0.97      0.98     12000\n",
      "          30       0.96      0.93      0.94     12000\n",
      "          31       0.91      0.95      0.93     12000\n",
      "          32       0.95      0.95      0.95     12000\n",
      "          33       0.96      0.97      0.96     12000\n",
      "          34       0.96      1.00      0.98     12000\n",
      "          35       0.83      0.77      0.80     12000\n",
      "          36       0.93      0.99      0.96     12000\n",
      "          37       0.75      0.82      0.78     12000\n",
      "          38       0.98      0.95      0.96     12000\n",
      "          39       0.93      0.91      0.92     12000\n",
      "          40       1.00      0.98      0.99     12000\n",
      "          41       0.97      0.99      0.98     12000\n",
      "          42       0.93      0.94      0.93     12000\n",
      "          43       0.90      0.96      0.93     12000\n",
      "          44       0.94      0.91      0.93     12000\n",
      "          45       0.88      0.94      0.91     12000\n",
      "          46       0.93      0.95      0.94     12000\n",
      "          47       0.97      0.99      0.98     12000\n",
      "          48       0.89      0.89      0.89     12000\n",
      "          49       0.92      0.94      0.93     12000\n",
      "          50       0.98      0.92      0.95     12000\n",
      "          51       0.95      0.93      0.94     12000\n",
      "          52       0.86      0.91      0.88     12000\n",
      "          53       0.89      0.95      0.92     12000\n",
      "          54       0.96      0.96      0.96     12000\n",
      "          55       0.87      0.80      0.83     12000\n",
      "          56       0.95      0.83      0.89     12000\n",
      "          57       0.96      0.97      0.97     12000\n",
      "          58       1.00      0.99      0.99     12000\n",
      "          59       0.97      0.94      0.95     12000\n",
      "          60       0.93      0.89      0.91     12000\n",
      "          61       0.99      0.98      0.98     12000\n",
      "          62       0.98      0.93      0.95     12000\n",
      "          63       0.99      0.92      0.95     12000\n",
      "          64       0.97      0.91      0.94     12000\n",
      "          65       0.93      0.99      0.95     12000\n",
      "          66       0.93      0.90      0.92     12000\n",
      "          67       1.00      0.98      0.99     12000\n",
      "          68       0.91      0.95      0.93     12000\n",
      "          69       0.80      0.90      0.85     12000\n",
      "          70       1.00      0.91      0.95     12000\n",
      "          71       0.91      0.96      0.94     12000\n",
      "          72       0.98      0.95      0.97     12000\n",
      "          73       0.97      0.96      0.97     12000\n",
      "          74       0.81      0.85      0.83     12000\n",
      "          75       0.99      0.98      0.98     12000\n",
      "          76       0.97      0.99      0.98     12000\n",
      "          77       0.89      0.95      0.92     12000\n",
      "          78       0.96      0.93      0.95     12000\n",
      "          79       0.83      0.90      0.86     12000\n",
      "          80       0.97      0.98      0.98     12000\n",
      "          81       0.94      0.94      0.94     12000\n",
      "          82       0.96      0.93      0.94     12000\n",
      "          83       1.00      0.99      1.00     12000\n",
      "          84       0.96      0.91      0.94     12000\n",
      "          85       0.96      0.97      0.97     12000\n",
      "          86       0.74      0.72      0.73     12000\n",
      "          87       0.80      0.65      0.71     12000\n",
      "          88       0.99      0.99      0.99     12000\n",
      "          89       0.81      0.88      0.85     12000\n",
      "          90       0.96      0.96      0.96     12000\n",
      "          91       0.74      0.84      0.79     12000\n",
      "          92       0.99      0.94      0.96     12000\n",
      "          93       1.00      1.00      1.00     12000\n",
      "          94       0.96      0.93      0.95     12000\n",
      "          95       0.96      0.95      0.95     12000\n",
      "          96       0.94      0.79      0.86     12000\n",
      "          97       0.95      0.97      0.96     12000\n",
      "          98       0.87      0.90      0.89     12000\n",
      "          99       0.82      0.90      0.86     12000\n",
      "         100       0.98      0.92      0.95     12000\n",
      "         101       0.93      0.97      0.95     12000\n",
      "         102       0.97      0.92      0.95     12000\n",
      "         103       0.95      0.97      0.96     12000\n",
      "         104       0.99      0.98      0.98     12000\n",
      "         105       0.93      0.89      0.91     12000\n",
      "         106       0.85      0.94      0.89      4736\n",
      "         107       0.88      0.92      0.90     12000\n",
      "         108       1.00      0.99      0.99     12000\n",
      "\n",
      "    accuracy                           0.93   1300736\n",
      "   macro avg       0.93      0.93      0.93   1300736\n",
      "weighted avg       0.93      0.93      0.93   1300736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(x)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41210f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5078/5078 [==============================] - 24s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.61      0.68      1499\n",
      "           1       0.88      0.70      0.78      1499\n",
      "           2       0.81      0.79      0.80      1499\n",
      "           3       0.85      0.95      0.90      1499\n",
      "           4       0.92      0.97      0.94      1499\n",
      "           5       0.99      0.91      0.95      1499\n",
      "           6       0.93      0.93      0.93      1499\n",
      "           7       0.95      0.93      0.94      1499\n",
      "           8       0.82      0.88      0.85      1499\n",
      "           9       0.79      0.78      0.79      1499\n",
      "          10       0.85      0.94      0.90      1499\n",
      "          11       0.92      0.98      0.95      1499\n",
      "          12       0.75      0.78      0.77      1499\n",
      "          13       0.77      0.91      0.84      1499\n",
      "          14       0.52      0.63      0.57      1499\n",
      "          15       0.89      0.97      0.93      1499\n",
      "          16       0.88      0.89      0.88      1499\n",
      "          17       0.89      0.76      0.82      1499\n",
      "          18       0.57      0.75      0.65      1499\n",
      "          19       0.61      0.17      0.27      1499\n",
      "          20       0.99      0.96      0.98      1499\n",
      "          21       0.87      0.92      0.90      1499\n",
      "          22       0.70      0.66      0.68      1499\n",
      "          23       0.95      0.99      0.97      1499\n",
      "          24       0.96      0.92      0.94      1499\n",
      "          25       0.67      0.52      0.58      1499\n",
      "          26       0.83      0.75      0.79      1499\n",
      "          27       0.55      0.74      0.63      1499\n",
      "          28       0.58      0.75      0.66      1499\n",
      "          29       0.97      0.83      0.89      1499\n",
      "          30       0.89      0.84      0.86      1499\n",
      "          31       0.72      0.92      0.81      1499\n",
      "          32       0.70      0.86      0.77      1499\n",
      "          33       0.83      0.85      0.84      1499\n",
      "          34       0.96      0.98      0.97      1499\n",
      "          35       0.70      0.63      0.66      1499\n",
      "          36       0.66      0.93      0.78      1499\n",
      "          37       0.62      0.75      0.68      1499\n",
      "          38       0.56      0.66      0.61      1499\n",
      "          39       0.64      0.70      0.67      1499\n",
      "          40       0.96      0.67      0.79      1499\n",
      "          41       0.83      0.84      0.84      1499\n",
      "          42       0.45      0.70      0.55      1499\n",
      "          43       0.78      0.88      0.83      1499\n",
      "          44       0.89      0.89      0.89      1499\n",
      "          45       0.68      0.85      0.76      1499\n",
      "          46       0.33      0.57      0.42      1499\n",
      "          47       0.78      0.80      0.79      1499\n",
      "          48       0.79      0.90      0.84      1499\n",
      "          49       0.73      0.86      0.79      1499\n",
      "          50       0.71      0.46      0.56      1499\n",
      "          51       0.75      0.61      0.67      1499\n",
      "          52       0.65      0.92      0.76      1499\n",
      "          53       0.79      0.84      0.81      1499\n",
      "          54       0.74      0.68      0.71      1499\n",
      "          55       0.78      0.72      0.75      1499\n",
      "          56       0.79      0.59      0.67      1499\n",
      "          57       0.90      0.95      0.93      1499\n",
      "          58       0.93      0.92      0.93      1499\n",
      "          59       0.83      0.70      0.76      1499\n",
      "          60       0.76      0.79      0.78      1499\n",
      "          61       0.94      0.72      0.82      1499\n",
      "          62       0.96      0.90      0.93      1499\n",
      "          63       0.73      0.46      0.56      1499\n",
      "          64       0.80      0.66      0.72      1499\n",
      "          65       0.86      0.96      0.91      1499\n",
      "          66       0.70      0.70      0.70      1499\n",
      "          67       0.83      0.36      0.50      1499\n",
      "          68       0.83      0.92      0.87      1499\n",
      "          69       0.50      0.74      0.59      1499\n",
      "          70       0.95      0.47      0.63      1499\n",
      "          71       0.44      0.65      0.52      1499\n",
      "          72       0.94      0.81      0.87      1499\n",
      "          73       0.71      0.84      0.77      1499\n",
      "          74       0.53      0.70      0.61      1499\n",
      "          75       0.93      0.75      0.83      1499\n",
      "          76       0.69      0.57      0.62      1499\n",
      "          77       0.71      0.63      0.67      1499\n",
      "          78       0.76      0.92      0.83      1499\n",
      "          79       0.74      0.82      0.78      1499\n",
      "          80       0.90      0.81      0.85      1499\n",
      "          81       0.81      0.65      0.72      1499\n",
      "          82       0.83      0.89      0.86      1499\n",
      "          83       0.98      0.77      0.86      1499\n",
      "          84       0.77      0.89      0.83      1499\n",
      "          85       0.83      0.94      0.88      1499\n",
      "          86       0.22      0.11      0.15      1499\n",
      "          87       0.60      0.60      0.60      1499\n",
      "          88       0.87      0.86      0.87      1499\n",
      "          89       0.70      0.83      0.76      1499\n",
      "          90       0.91      0.94      0.92      1499\n",
      "          91       0.56      0.75      0.64      1499\n",
      "          92       0.88      0.54      0.67      1499\n",
      "          93       0.99      1.00      0.99      1499\n",
      "          94       0.73      0.86      0.79      1499\n",
      "          95       0.87      0.86      0.87      1499\n",
      "          96       0.85      0.62      0.71      1499\n",
      "          97       0.69      0.78      0.73      1499\n",
      "          98       0.83      0.80      0.82      1499\n",
      "          99       0.69      0.79      0.74      1499\n",
      "         100       0.91      0.70      0.79      1499\n",
      "         101       0.62      0.49      0.55      1499\n",
      "         102       0.72      0.21      0.32      1499\n",
      "         103       0.71      0.37      0.49      1499\n",
      "         104       0.95      0.79      0.86      1499\n",
      "         105       0.47      0.49      0.48      1499\n",
      "         106       0.27      0.35      0.31       591\n",
      "         107       0.71      0.75      0.73      1499\n",
      "         108       0.91      0.87      0.89      1499\n",
      "\n",
      "    accuracy                           0.76    162483\n",
      "   macro avg       0.77      0.76      0.75    162483\n",
      "weighted avg       0.77      0.76      0.75    162483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(xt)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(yt,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c94065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
